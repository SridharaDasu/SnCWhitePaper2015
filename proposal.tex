\section{Proposed Solutions}

Vision for US CMS computing environment for 2017-21 and beyond, comprises
strengthening of its current FNAL based Tier-1 and seven university based
Tier-2s, all functioning as portals to the nation-wide research and commercial
clouds. These service providers will also enable all US universities to 
connect to the seamless CMS cloud, by providing them suitable 
"headnodes", democratizing access CMS computing. The resource 
provisioning targets steady-state operations level  at the owned facilities in
FNAL T1 and University Tier-2s, while peak fluctuations are handled by 
overflowing to the clouds. Non-owned opportunistic resources at all 
campuses are integrated in the CMS cloud.

The advantage of strengthening the university sites is multi-fold:
\begin{itemize}
\item Each university group brings unique experience and expertise to bear
\begin{itemize}
\item MIT: Dynamic data management and production operations expertise
\item Nebraska: Dr. Bockleman et al, brought in numerous innovations to CMS middleware
\item San Diego: Connections to SDSC, Connections to core CMS software developers
\item Wisconsin: Connections to HT-Condor and OSG core-developers
\end{itemize}
\item Connection to strong physics groups at the universities
\begin{itemize}
\item Student and postdoc physics analysts exercise the system providing
appropriate usecases for tuning. 
\item Faculty collaborations at the University level can bring in additional
campus or cloud resources
\end{itemize}
\item Cost of infrastructure is subsidized at the Universities.
\item Cost of personnel is also lower.
\item Friendly competition amongst the sites results in increased productivity.
\end{itemize}

\subsection{Storage Resources}

The storage resource requirements are estimated by scaling current data set 
of 30 fb$^{-1}$ acquired in 2010-2012 (Run-1) and 2015 (Early Run-2) to the
full dataset of 300 fb$^{-1}$.  MiniAOD usage reduces the needed resources
significantly but poses, as indicated earlier.  However, it poses data versioning
issues resulting in multipliers.  The user data storage space, is somewhat
ill defined, but it does scales with the luminosity. We use the current usage
at US T2s with the luminosity ratio to estimate this.  

\begin{itemize}
\item 20-30 PB for MiniAOD (disk for one copy)
\begin{itemize}
\item 40 PB for MiniAOD for analysts including replication.
\end{itemize}
\item 10 PB for AOD (for a fraction of datasets)
\begin{itemize}
\item 10 PB fraction should satisfy users requiring full AOD access.
\end{itemize}
\item Currently user space at T2s is typically 0.5 PB with 10\% of data acquired, i.e., 30 fb$^{-1}$.
Projecting to full dataset one gets 5 PB per T2.
\begin{itemize} 
\item 30 PB of storage for user data (non-archived)
\end{itemize}
\item CMS upgrade design tuning requires custom simulations, which are much more
storage resource intensive than Run-2/3 data.  The pileup level will be an order of
magnitude higher.
\begin{itemize} 
\item 20 PB of storage for upgrade data
\end{itemize} 
\item Analysis workflows and improvements to MiniAOD require access to RAW data
\begin{itemize}
\item 60 PB for RAW (tape archive) for full 300 fb$^{-1}$
\end{itemize} 
\begin{itemize}
\item 10 PB for RAW for special analyses cache
\end{itemize}
\item Total Disk Storage:  110 PB
\item Proposed Approximate Disk Storage Distribution: 30 PB disk and 110 PB tape at T1 and 12 PB at each at seven US T2s
\end{itemize}

\subsection{Computing Resources}

Unlike for Run-1, the Run-2/3 Tier-2 compute resource provisioning details are not 
concretely specified to allow local optimization based on cloud and opportunistic
resource availability. However, the non-owned storage resource use is not yet
well defined. Therefore, it is visualized that much of the storage, which is pared 
down to the minimum with improvements made in LS1, is owned and operated at 
the Tier-1s and Tier-2s. 

The CPU requirements are estimated, in units of number of slots needed, by scaling current 
usage up by a factor of 10 accounting for increase in expected integrated luminosity.  The 
increase in complexity of analysis is assumed to be compensated by the improved 
framework job efficiency and advances in computing power of individual machines.
Use of CMS data federation across the wide-area network at owned resource sites
and the clouds in general, opens up the possibility of provisioning needed compute
resources in a flexible way depending on cost/benefit.  However, it is visualized
that a fraction of resources are housed at existing T2s.

\begin{itemize}
\item Currently 30,000 jobs, averaged over the past month, run at the seven 
US T2s equally split between production and analysis and 10,000 production
jobs at FNAL T1.
\item Bulk of the 15,000 analysis jobs running at Tier-2s recently are 
identified as 13-TeV MC jobs.  The MC for 2015 was generated with the
anticipation that we collect 10 fb$^{-1}$ this year.  Unfortunately,
we only collected 2 fb$^{-1}$.  Nevertheless, the analysis effort 
presently is equivalent to 10 fb$^{-1}$.
\item Therefore, scaling by luminosity, 300 fb$^{-1}$ vs 10 fb$^{-1}$ collected 
to date, we should expect to support about 900,000 jobs at T2s at steady state 
and 300,000 jobs at T1.
\item Proposed job slot availability: 300,000 for production at T1 and 100,000 through each of the seven US T2s.
\end{itemize}

\subsection{Network Resources}

The network bandwidth requirement will also scale with increased data size 
and wide-area distributed computing.  Typically sites are connected through
100 Gbps network presently, and we visualize multi-100 Gbps connections
in the coming years and that they are funded through separate initiatives.

\subsection{Non-owned Resources Usage Plan}
\noindent{(Kevin)}
Need to explain why we need to provide infrastructure.
Why not amazon computing cards for all.
\subsubsection{Opportunistic}
\subsubsection{Commercial}

\subsection{Middleware / Software}

\subsection{Support personnel roles}

Two persons at each facility are necessary to provide full coverage.  However,
recent experience indicates that about 30-50\% of those person's effort can be
freed up for other work. Most of the effective people involved in CMS computing
are former HEP physicists, who have now become experts in computing. They
are able to provide wide-ranging expertise in physics software development.
The additional services we expect Tier-2 personnel to provide are in the areas:
\begin{itemize}
\item Support for non-Tier-2 university portals to CMS cloud
\begin{itemize}
\item We expect each Tier-2 to support about 7 universities in their neighborhood.
\end{itemize}
\item Computing services for CMS upgrades and research to address future needs
\begin{itemize}
\item Development of simulation program for upgrade detectors
\item Production of simulation data for upgrade detectors
\item Participation in computing research
\item Participation in DIANA/HEP and other community wide computing projects for future.
\end{itemize}
\end{itemize} 

\subsection{Physics Driven Datapaths}
\noindent{(Frank)}
Ideas for processing throughput increase
\begin{verbatim}
Datasets of trigger paths.
On-demand processing for some paths, etc.
\end{verbatim}
